# -*- coding: utf-8 -*-
"""LLM for booking train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11HJLTroEz1EGRg6yGFyWi3AMrjrXLsHL
"""

! pip install -U datasets

!pip install -U transformers datasets evaluate --quiet

!pip install rouge_score

import os
import torch
import random
import numpy as np

from datasets import load_dataset
import transformers
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, AutoModelForTokenClassification, pipeline
import evaluate

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)

os.environ["WANDB_DISABLED"] = "true"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

dataset = load_dataset("GEM/schema_guided_dialog")

train_dataset = dataset["train"].select(range(5000))
eval_dataset = dataset["validation"].select(range(1000))

print("Колонки:", train_dataset.column_names)
print("Пример:")
print("Prompt:\n", train_dataset[0]["prompt"])
print("Target:\n", train_dataset[0]["target"])

model_name = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Загрузка модели NER для извлечения слотов
ner_model_name = "dslim/bert-base-NER"
ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name)
ner_model = AutoModelForTokenClassification.from_pretrained(ner_model_name)
ner_pipeline = pipeline("ner", model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy="simple")

# Используем linearized_input как вход в модель
def preprocess_function(example):
    input_text = example["linearized_input"]

    inputs = tokenizer(
        input_text,
        max_length=128,
        padding="max_length",
        truncation=True,
    )
    targets = tokenizer(
        example["target"],
        max_length=64,
        padding="max_length",
        truncation=True,
    )
    inputs["labels"] = [
        (label if label != tokenizer.pad_token_id else -100)
        for label in targets["input_ids"]
    ]
    return inputs

train_tok = train_dataset.map(preprocess_function, batched=False)
eval_tok = eval_dataset.map(preprocess_function, batched=False)

print("Токенизация прошла успешно, пример токенов:")
print(train_tok[0])

bleu = evaluate.load("bleu")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = torch.argmax(torch.tensor(logits), dim=-1)

    pred_str = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)

    pred_tokens = [p.split() for p in pred_str]
    label_tokens = [[l.split()] for l in label_str]

    return {"bleu": bleu.compute(predictions=pred_tokens, references=label_tokens)["bleu"]}

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=1,
    logging_dir="./logs",
    logging_steps=50,
    eval_strategy="no",
    save_strategy="no",
    report_to="none",
    remove_unused_columns=True,
    fp16=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tok,
    eval_dataset=eval_tok,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

torch.cuda.empty_cache()
trainer.train()

model.save_pretrained("./t5-small-finetuned")
tokenizer.save_pretrained("./t5-small-finetuned")

!zip -r t5-small-finetuned.zip t5-small-finetuned
from google.colab import files
files.download("t5-small-finetuned.zip")